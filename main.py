import streamlit as st
import json
import base64
import re
from prompt_engine import generate_experiment_plan

def sanitize_text(text):
    if not text or not isinstance(text, str):
        return ""
    text = text.replace("\n", " ").replace("\r", " ").replace("\t", " ")
    text = re.sub(r"\s+", " ", text)
    return text.strip()

def safe_display(text, method=st.info):
    clean_text = sanitize_text(text)
    method(clean_text)

def extract_json(text):
    try:
        match = re.search(r"\{[\s\S]+\}", text)
        return match.group(0) if match else text
    except:
        return text

def remove_units_from_text(text, unit):
    if not text or not unit.strip():
        return text
    escaped_unit = re.escape(unit.strip())
    return re.sub(rf"(\d+\.?\d*)\s*{escaped_unit}", r"\1", text)

def insert_units_in_goal(text, unit):
    if not text or not unit.strip():
        return text
    return re.sub(r"(\d+\.?\d*)", r"\1" + unit, text)

# --- Page Setup ---
st.set_page_config(page_title="A/B Test Architect", layout="wide")
st.title("\U0001F9EA AI-Powered A/B Test Architect")
st.markdown("Use Groq + LLMs to design smarter experiments from fuzzy product goals.")

# --- Product Context ---
st.header("\U0001F9E0 Product Context")
product_type = st.radio("Product Type *", ["SaaS", "Consumer App", "E-commerce", "Marketplace", "Gaming", "Other"], horizontal=True)
user_base = st.radio("User Base Size (DAU) *", ["< 10K", "10K‚Äì100K", "100K‚Äì1M", "> 1M"], horizontal=True)
metric_focus = st.radio("Primary Metric Focus *", ["Activation", "Retention", "Monetization", "Engagement", "Virality"], horizontal=True)
product_notes = st.text_area("Anything unique about your product or users?", placeholder="e.g. drop-off at pricing, seasonality, power users...")

# --- Metric Objective ---
st.markdown("## \U0001F3AF Metric Improvement Objective")
exact_metric = st.text_input("\U0001F3AF Metric to Improve * (e.g. Activation Rate, ARPU, DAU/MAU)")
metric_unit = st.text_input("\U0001F4A0 Metric Unit (e.g. %, $, secs, count)", value="%")
current_value_raw = st.text_input("\U0001F4C9 Current Metric Value * (numerical only)")
target_value_raw = st.text_input("üöÄ Target Metric Value * (numerical only)")

# --- Generate Plan ---
if st.button("Generate Plan") or "output" not in st.session_state:
    missing = []
    if not product_type: missing.append("Product Type")
    if not user_base: missing.append("User Base Size")
    if not metric_focus: missing.append("Primary Metric Focus")
    if not exact_metric.strip(): missing.append("Metric to Improve")
    if not current_value_raw.strip(): missing.append("Current Value")
    if not target_value_raw.strip(): missing.append("Target Value")
    if not metric_unit.strip(): missing.append("Metric Unit")

    if missing:
        st.warning("Please fill all required fields: " + ", ".join(missing))
        st.stop()

    try:
        current = float(current_value_raw)
        target = float(target_value_raw)
        expected_lift = round(target - current, 4)
        mde = round(abs((target - current) / current), 4) if current != 0 else 0.0
    except ValueError:
        st.error("Metric values must be numeric.")
        st.stop()

    st.session_state.current = current
    st.session_state.target = target
    st.session_state.auto_goal = f"I want to improve {exact_metric} from {current} to {target}."
    st.session_state.context = {
        "type": product_type,
        "users": user_base,
        "metric": metric_focus,
        "notes": product_notes,
        "exact_metric": exact_metric,
        "current_value": current,
        "target_value": target,
        "expected_lift": expected_lift,
        "minimum_detectable_effect": round(mde * 100, 2),
        "metric_unit": metric_unit.strip()
    }

    output = generate_experiment_plan(st.session_state.auto_goal, st.session_state.context)
    st.session_state.output = output
    st.session_state.hypothesis_confirmed = False
    st.session_state.selected_index = None

# --- Display Output ---
if "output" in st.session_state:
    raw_output = extract_json(st.session_state.output)
    try:
        plan = json.loads(raw_output)
    except Exception as e:
        st.error(f"\u274C Could not parse JSON: {e}")
        st.code(raw_output)
        st.stop()

    unit = " " + st.session_state.context.get("metric_unit", "").strip()
    st.markdown("## ‚úçÔ∏è Inferred Product Goal")
    safe_display(st.session_state.auto_goal, method=st.info)

    st.subheader("\U0001F9E9 Problem Statement")
    problem_statement = remove_units_from_text(plan.get("problem_statement", ""), unit)
    safe_display(problem_statement or "\u26A0Ô∏è Problem statement not generated by the model.")

    st.subheader("\U0001F9EA Choose a Hypothesis")
    hypotheses = plan.get("hypotheses", [])
    if not hypotheses:
        st.warning("No hypotheses found in the generated plan.")
    else:
        for i, h in enumerate(hypotheses):
            hypo = h.get("hypothesis") if isinstance(h, dict) else str(h)
            with st.expander(f"H{i+1}: {hypo}", expanded=(st.session_state.get("selected_index") == i)):
                if st.button(f"‚úÖ Select H{i+1}", key=f"select_{i}"):
                    st.session_state.selected_index = i
                    st.session_state.hypothesis_confirmed = True
                    st.experimental_rerun()

# --- Polished PRD ---
if (
    "output" in st.session_state and
    st.session_state.get("hypothesis_confirmed") and
    st.session_state.get("selected_index") is not None
):
    i = st.session_state.selected_index
    plan = json.loads(extract_json(st.session_state.output))
    selected_hypo_obj = plan.get("hypotheses", [{}])[i]
    selected_hypo = selected_hypo_obj.get("hypothesis", "N/A")
    effort = plan.get("effort", [{}])[i].get("effort", "N/A")
    variant = plan.get("variants", [{}])[i]
    control = variant.get("control", "Not specified")
    variation = variant.get("variation", "Not specified")
    rationale_list = plan.get("hypothesis_rationale", [])
    rationale = rationale_list[i] if i < len(rationale_list) else "Not available"
    if isinstance(rationale, dict):
        rationale = rationale.get("rationale", "Not available")
    rationale = sanitize_text(rationale)
    criteria = plan.get("success_criteria", {})
    segments = plan.get("segments", [])
    metrics = plan.get("metrics", [])
    teams = plan.get("team_involved", [])

    # Success Criteria Cleanup
    try:
        conf_level = float(criteria.get("confidence_level", 0))
        conf_display = f"{round(conf_level)}%" if conf_level > 1 else f"{round(conf_level * 100)}%"
    except:
        conf_display = "N/A"

    expected_lift = criteria.get("expected_lift", "N/A")
    try:
        expected_lift_val = float(expected_lift)
        expected_lift_str = f"{expected_lift_val}{unit}"
    except:
        expected_lift_str = expected_lift

    try:
        mde = float(criteria.get("MDE", 0))
        mde_display = f"{round(mde)}%" if mde > 1 else f"{round(mde * 100)}%"
    except:
        mde_display = "N/A"

    test_duration = criteria.get("estimated_test_duration", "N/A")
    risks = [sanitize_text(r) for r in plan.get("risks_and_assumptions", []) if isinstance(r, str)]
    steps = [sanitize_text(s) for s in plan.get("next_steps", []) if isinstance(s, str)]

    export = f"""# üìÑ Experiment PRD: {selected_hypo[:60]}

## üî© Problem Statement
{problem_statement}

## üåü Objective
Increase {exact_metric} from {st.session_state.current} to {st.session_state.target} by launching a targeted experiment.

## üß™ Hypothesis
{selected_hypo}

## üîÄ Test Variants
- **Control**: {control}
- **Variation**: {variation}

## üí° Rationale
{rationale}

## üìä Success Criteria
| Metric                     | Value                |
|---------------------------|----------------------|
| Confidence Level          | {conf_display}       |
| Expected Lift             | {expected_lift_str}  |
| Minimum Detectable Effect | {mde_display}        |
| Test Duration             | {test_duration} days |

## üìà Metrics to Track
"""
    for metric in metrics:
        export += f"- **{metric.get('name', 'Unnamed')}**: {metric.get('formula', 'N/A')}\n"

    export += "\n## üë• Segments for Breakdown\n"
    for s in segments:
        export += f"- {s}\n"

    export += f"\n## ‚öôÔ∏è Implementation Effort\n- **Effort**: {effort}\n- **Teams**: {', '.join(teams)}\n"

    export += "\n## ‚ö†Ô∏è Risks and Assumptions\n"
    for r in risks:
        export += f"- {r}\n"

    export += "\n## ‚úÖ Next Steps\n"
    for s in steps:
        export += f"- {s}\n"

    st.download_button(
        label="üìÖ Download Polished PRD",
        data=export,
        file_name="experiment_prd.txt",
        mime="text/plain"
    )
